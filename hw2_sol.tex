\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\geometry{a4paper}
\usepackage{graphicx}
\usepackage{fancyhdr}
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt}
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape}

\title{CS294-1 Programming Assignment 2}
\author{Richard Hwang, David Huang}

\begin{document}
\maketitle

\section{Introduction}
Linear regression is applied at scale to amazon.com review data.  Given the tokenized version of Mark Dredze's sentiment data, we performed linear regression, cross validating and creating ROC and lift plots.

\section{Data Processing}
Special care had to be taken in constructing our data matrix.  Given the provided data files, we constructed $X$ in blocks, processing $block\_size$ reviews at a time, and then concatenating all the sparse matrices in the end.  We moved along the tokens files.  If we encountered a \textless review\textgreater tag, we knew we had started a new review.  Likewise, if we encountered a \textless rating\textgreater tag, we knew the next token had to be a rating.  For normal words, we kept a map of tokens to counts.  In this way, we constructed $X$.  We performed this transformation in blocks because DAVID HELLPP

\section{Regression}
Given $X$ and $Y$, we performed linear regression with $L_2$ loss function along with Lasso regularization by using stochastic gradient descent.  This means we updated our weight vector $\beta$ according to:\\\\
\centerline{$\beta_{t+1} \leftarrow \beta_{t} - \gamma \nabla L(\beta, x, y)$}
where\\\\
\centerline{$\nabla L(\beta, x, y) = E(2x^T x \beta - 2 y^T x) + 2 \lambda \beta$}
We repeated these updates for TODO iterations and plotted error rates versus iteration to determine reasonable $i$.  To evaluate our model, we performed 10-fold cross-validation, splitting up our data into 10 sections.  We then trained on 9 sections and evaluated on the last for all possible rotations.  In each iteration, we also generated ROC plots.  This was done by first sorting predictions in decreasing order.  Then, for each point, we compute the false positive rate (FPR) and true positive rate (TPR) as if the point were the threshold of classification.  Whenever we notice the FPR hits a "tick" on the graph (for example, going from <0.01 to >0.01), we record the corresponding TPR.  We average the TPR across all cross-validation iterations to obtain the area under the curve (AUC).    We also average the ROC plot across all folds.  

\section{Results}


\section{Performance}


\end{document}
